defaults:
  - _self_

# Model configuration
model:
  model_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # Will use 4-bit quantization to fit 8GB GPU
  load_in_4bit: true
  use_peft: true
  device_map: "auto"

# Dataset configuration
dataset:
  dataset_name: "iamtarun/python_code_instructions_18k_alpaca"
  test_size: 0.1
  random_seed: 42
  max_samples: null  # Set to number if you want to limit dataset size / null
  max_seq_length: 512

# Training configuration
training:
  output_dir: "./outputs"
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 2
  num_train_epochs: 3
  learning_rate: 2e-4
  logging_steps: 10
  save_steps: 200
  eval_steps: 200
  optim: "adamw_torch_fused"
  save_total_limit: 3
  fp16: true
  report_to: "none"

  # LoRA configuration
  lora:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj"]
    bias: "none"
    task_type: "CAUSAL_LM"